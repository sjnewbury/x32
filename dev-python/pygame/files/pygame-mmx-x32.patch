--- ./src/scale_mmx64.c.orig	2011-12-31 23:00:00.000000000 +0000
+++ ./src/scale_mmx64.c	2013-08-20 07:15:58.422670509 +0000
@@ -41,13 +41,13 @@
 /* These functions implement an area-averaging shrinking filter in the X-dimension.
  */
 void
-filter_shrink_X_MMX(Uint8 *srcpix, Uint8 *dstpix, int height, int srcpitch, int dstpitch, int srcwidth, int dstwidth)
+filter_shrink_X_MMX(Uint8 *srcpix, Uint8 *dstpix, int64_t height, int64_t srcpitch, int64_t dstpitch, int64_t srcwidth, int64_t dstwidth)
 {
-    int srcdiff = srcpitch - (srcwidth * 4);
-    int dstdiff = dstpitch - (dstwidth * 4);
+    int64_t srcdiff = srcpitch - (srcwidth * 4);
+    int64_t dstdiff = dstpitch - (dstwidth * 4);
 
-    int xspace = 0x04000 * srcwidth / dstwidth; /* must be > 1 */
-    int xrecip = 0x40000000 / xspace;
+    int64_t xspace = 0x04000 * srcwidth / dstwidth; /* must be > 1 */
+    int64_t xrecip = 0x40000000 / xspace;
     long long One64 = 0x4000400040004000ULL;
     long long srcdiff64 = srcdiff;
     long long dstdiff64 = dstdiff;
@@ -129,13 +129,13 @@
 }
 
 void
-filter_shrink_X_SSE(Uint8 *srcpix, Uint8 *dstpix, int height, int srcpitch, int dstpitch, int srcwidth, int dstwidth)
+filter_shrink_X_SSE(Uint8 *srcpix, Uint8 *dstpix, int64_t height, int64_t srcpitch, int64_t dstpitch, int64_t srcwidth, int64_t dstwidth)
 {
-    int srcdiff = srcpitch - (srcwidth * 4);
-    int dstdiff = dstpitch - (dstwidth * 4);
+    int64_t srcdiff = srcpitch - (srcwidth * 4);
+    int64_t dstdiff = dstpitch - (dstwidth * 4);
 
-    int xspace = 0x04000 * srcwidth / dstwidth; /* must be > 1 */
-    int xrecip = 0x40000000 / xspace;
+    int64_t xspace = 0x04000 * srcwidth / dstwidth; /* must be > 1 */
+    int64_t xrecip = 0x40000000 / xspace;
     long long One64 = 0x4000400040004000ULL;
     long long srcdiff64 = srcdiff;
     long long dstdiff64 = dstdiff;
@@ -194,13 +194,13 @@
 /* These functions implement an area-averaging shrinking filter in the Y-dimension.
  */
 void
-filter_shrink_Y_MMX(Uint8 *srcpix, Uint8 *dstpix, int width, int srcpitch, int dstpitch, int srcheight, int dstheight)
+filter_shrink_Y_MMX(Uint8 *srcpix, Uint8 *dstpix, int64_t width, int64_t srcpitch, int64_t dstpitch, int64_t srcheight, int64_t dstheight)
 {
     Uint16 *templine;
-    int srcdiff = srcpitch - (width * 4);
-    int dstdiff = dstpitch - (width * 4);
-    int yspace = 0x4000 * srcheight / dstheight; /* must be > 1 */
-    int yrecip = 0x40000000 / yspace;
+    int64_t srcdiff = srcpitch - (width * 4);
+    int64_t dstdiff = dstpitch - (width * 4);
+    int64_t yspace = 0x4000 * srcheight / dstheight; /* must be > 1 */
+    int64_t yrecip = 0x40000000 / yspace;
     long long One64 = 0x4000400040004000ULL;
 
     /* allocate and clear a memory area for storing the accumulator line */
@@ -301,13 +301,13 @@
 }
 
 void
-filter_shrink_Y_SSE(Uint8 *srcpix, Uint8 *dstpix, int width, int srcpitch, int dstpitch, int srcheight, int dstheight)
+filter_shrink_Y_SSE(Uint8 *srcpix, Uint8 *dstpix, int64_t width, int64_t srcpitch, int64_t dstpitch, int64_t srcheight, int64_t dstheight)
 {
     Uint16 *templine;
-    int srcdiff = srcpitch - (width * 4);
-    int dstdiff = dstpitch - (width * 4);
-    int yspace = 0x4000 * srcheight / dstheight; /* must be > 1 */
-    int yrecip = 0x40000000 / yspace;
+    int64_t srcdiff = srcpitch - (width * 4);
+    int64_t dstdiff = dstpitch - (width * 4);
+    int64_t yspace = 0x4000 * srcheight / dstheight; /* must be > 1 */
+    int64_t yrecip = 0x40000000 / yspace;
     long long One64 = 0x4000400040004000ULL;
 
     /* allocate and clear a memory area for storing the accumulator line */
@@ -383,17 +383,17 @@
 /* These functions implement a bilinear filter in the X-dimension.
  */
 void
-filter_expand_X_MMX(Uint8 *srcpix, Uint8 *dstpix, int height, int srcpitch, int dstpitch, int srcwidth, int dstwidth)
+filter_expand_X_MMX(Uint8 *srcpix, Uint8 *dstpix, int64_t height, int64_t srcpitch, int64_t dstpitch, int64_t srcwidth, int64_t dstwidth)
 {
-    int *xidx0, *xmult0, *xmult1;
-    int x, y;
-    int factorwidth = 8;
+    intptr_t *xidx0, *xmult0, *xmult1;
+    int64_t x, y;
+    int64_t factorwidth = 8;
 
     /* Allocate memory for factors */
     xidx0 = malloc(dstwidth * 4);
     if (xidx0 == 0) return;
-    xmult0 = (int *) malloc(dstwidth * factorwidth);
-    xmult1 = (int *) malloc(dstwidth * factorwidth);
+    xmult0 = (intptr_t *) malloc(dstwidth * factorwidth);
+    xmult1 = (intptr_t *) malloc(dstwidth * factorwidth);
     if (xmult0 == 0 || xmult1 == 0)
     {
         free(xidx0);
@@ -404,8 +404,8 @@
     /* Create multiplier factors and starting indices and put them in arrays */
     for (x = 0; x < dstwidth; x++)
     {
-        int xm1 = 0x100 * ((x * (srcwidth - 1)) % dstwidth) / dstwidth;
-        int xm0 = 0x100 - xm1;
+        int64_t xm1 = 0x100 * ((x * (srcwidth - 1)) % dstwidth) / dstwidth;
+        int64_t xm0 = 0x100 - xm1;
         xidx0[x] = x * (srcwidth - 1) / dstwidth;
         xmult1[x*2]   = xm1 | (xm1 << 16);
         xmult1[x*2+1] = xm1 | (xm1 << 16);
@@ -418,9 +418,9 @@
     {
         Uint8 *srcrow0 = srcpix + y * srcpitch;
         Uint8 *dstrow = dstpix + y * dstpitch;
-        int *xm0 = xmult0;
-		int *xm1 = xmult1;
-        int *x0 = xidx0;
+        intptr_t *xm0 = xmult0;
+		intptr_t *xm1 = xmult1;
+        intptr_t *x0 = xidx0;
         asm __volatile__( " /* MMX code for inner loop of X bilinear filter */ "
              " movl             %5,      %%ecx;           "
              " pxor          %%mm0,      %%mm0;           "
@@ -431,8 +431,13 @@
              " add              $8,         %0;           "
              " movq           (%1),      %%mm2;           " /* load mult1 */
              " add              $8,         %1;           "
+#if (__LP64__)
              " movd   (%4,%%rax,4),      %%mm4;           "
              " movd  4(%4,%%rax,4),      %%mm5;           "
+#else
+             " movd   (%4,%%eax,4),      %%mm4;           "
+             " movd  4(%4,%%eax,4),      %%mm5;           "
+#endif
              " punpcklbw     %%mm0,      %%mm4;           "
              " punpcklbw     %%mm0,      %%mm5;           "
              " pmullw        %%mm1,      %%mm4;           "
@@ -458,17 +463,17 @@
 }
 
 void
-filter_expand_X_SSE(Uint8 *srcpix, Uint8 *dstpix, int height, int srcpitch, int dstpitch, int srcwidth, int dstwidth)
+filter_expand_X_SSE(Uint8 *srcpix, Uint8 *dstpix, int64_t height, int64_t srcpitch, int64_t dstpitch, int64_t srcwidth, int64_t dstwidth)
 {
-    int *xidx0, *xmult0, *xmult1;
-    int x, y;
-    int factorwidth = 8;
+    intptr_t *xidx0, *xmult0, *xmult1;
+    int64_t x, y;
+    int64_t factorwidth = 8;
 
     /* Allocate memory for factors */
     xidx0 = malloc(dstwidth * 4);
     if (xidx0 == 0) return;
-    xmult0 = (int *) malloc(dstwidth * factorwidth);
-    xmult1 = (int *) malloc(dstwidth * factorwidth);
+    xmult0 = (intptr_t *) malloc(dstwidth * factorwidth);
+    xmult1 = (intptr_t *) malloc(dstwidth * factorwidth);
     if (xmult0 == 0 || xmult1 == 0)
     {
         free(xidx0);
@@ -479,8 +484,8 @@
     /* Create multiplier factors and starting indices and put them in arrays */
     for (x = 0; x < dstwidth; x++)
     {
-        int xm1 = 0x100 * ((x * (srcwidth - 1)) % dstwidth) / dstwidth;
-        int xm0 = 0x100 - xm1;
+        intptr_t xm1 = 0x100 * ((x * (srcwidth - 1)) % dstwidth) / dstwidth;
+        intptr_t xm0 = 0x100 - xm1;
         xidx0[x] = x * (srcwidth - 1) / dstwidth;
         xmult1[x*2]   = xm1 | (xm1 << 16);
         xmult1[x*2+1] = xm1 | (xm1 << 16);
@@ -493,9 +498,9 @@
     {
         Uint8 *srcrow0 = srcpix + y * srcpitch;
         Uint8 *dstrow = dstpix + y * dstpitch;
-        int *xm0 = xmult0;
-		int *xm1 = xmult1;
-        int *x0 = xidx0;
+        intptr_t *xm0 = xmult0;
+		intptr_t *xm1 = xmult1;
+        intptr_t *x0 = xidx0;
         asm __volatile__( " /* MMX code for inner loop of X bilinear filter */ "
              " movl             %5,      %%ecx;           "
              " pxor          %%mm0,      %%mm0;           "
@@ -506,8 +511,13 @@
              " add              $8,         %0;           "
              " movq           (%1),      %%mm2;           " /* load mult1 */
              " add              $8,         %1;           "
+#if (__LP64__)
              " movd   (%4,%%rax,4),      %%mm4;           "
              " movd  4(%4,%%rax,4),      %%mm5;           "
+#else
+             " movd   (%4,%%eax,4),      %%mm4;           "
+             " movd  4(%4,%%eax,4),      %%mm5;           "
+#endif
              " punpcklbw     %%mm0,      %%mm4;           "
              " punpcklbw     %%mm0,      %%mm5;           "
              " pmullw        %%mm1,      %%mm4;           "
@@ -535,17 +545,17 @@
 /* These functions implement a bilinear filter in the Y-dimension
  */
 void
-filter_expand_Y_MMX(Uint8 *srcpix, Uint8 *dstpix, int width, int srcpitch, int dstpitch, int srcheight, int dstheight)
+filter_expand_Y_MMX(Uint8 *srcpix, Uint8 *dstpix, int64_t width, int64_t srcpitch, int64_t dstpitch, int64_t srcheight, int64_t dstheight)
 {
-    int y;
+    int64_t y;
 
     for (y = 0; y < dstheight; y++)
     {
-        int yidx0 = y * (srcheight - 1) / dstheight;
+        int64_t yidx0 = y * (srcheight - 1) / dstheight;
         Uint8 *srcrow0 = srcpix + yidx0 * srcpitch;
         Uint8 *srcrow1 = srcrow0 + srcpitch;
-        int ymult1 = 0x0100 * ((y * (srcheight - 1)) % dstheight) / dstheight;
-        int ymult0 = 0x0100 - ymult1;
+        int64_t ymult1 = 0x0100 * ((y * (srcheight - 1)) % dstheight) / dstheight;
+        int64_t ymult0 = 0x0100 - ymult1;
         Uint8 *dstrow = dstpix + y * dstpitch;
         asm __volatile__( " /* MMX code for inner loop of Y bilinear filter */ "
              " movl          %5,      %%ecx;                      "
@@ -581,17 +591,17 @@
 }
 
 void
-filter_expand_Y_SSE(Uint8 *srcpix, Uint8 *dstpix, int width, int srcpitch, int dstpitch, int srcheight, int dstheight)
+filter_expand_Y_SSE(Uint8 *srcpix, Uint8 *dstpix, int64_t width, int64_t srcpitch, int64_t dstpitch, int64_t srcheight, int64_t dstheight)
 {
-    int y;
+    int64_t y;
 
     for (y = 0; y < dstheight; y++)
     {
-        int yidx0 = y * (srcheight - 1) / dstheight;
+        int64_t yidx0 = y * (srcheight - 1) / dstheight;
         Uint8 *srcrow0 = srcpix + yidx0 * srcpitch;
         Uint8 *srcrow1 = srcrow0 + srcpitch;
-        int ymult1 = 0x0100 * ((y * (srcheight - 1)) % dstheight) / dstheight;
-        int ymult0 = 0x0100 - ymult1;
+        int64_t ymult1 = 0x0100 * ((y * (srcheight - 1)) % dstheight) / dstheight;
+        int64_t ymult0 = 0x0100 - ymult1;
         Uint8 *dstrow = dstpix + y * dstpitch;
         asm __volatile__( " /* MMX code for inner loop of Y bilinear filter */ "
              " movl          %5,      %%ecx;                      "
